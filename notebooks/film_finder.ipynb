{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T02:30:49.406429Z",
     "start_time": "2019-12-24T02:30:49.401769Z"
    }
   },
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import seaborn as sns\n",
    "import string\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import pairwise_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open CSV and Clean Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T02:30:50.213258Z",
     "start_time": "2019-12-24T02:30:49.408855Z"
    }
   },
   "outputs": [],
   "source": [
    "# Open dataframe\n",
    "movie_df = pd.read_csv('../csvs/wiki_movie_plots.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T02:30:50.244069Z",
     "start_time": "2019-12-24T02:30:50.215992Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "movie_df = movie_df.drop_duplicates(subset = ['Wiki Page'], keep = 'first').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T02:30:50.251272Z",
     "start_time": "2019-12-24T02:30:50.246946Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove apostrophes\n",
    "remove_apostrophes = lambda x: x.replace('\\'', '')\n",
    "\n",
    "# Keep only letters\n",
    "remove_numbers = lambda x: ' '.join(re.sub('\\w*\\d\\w*', ' ', x).split())\n",
    "\n",
    "# Remove new line characters\n",
    "no_new_line = lambda x: x.replace('\\n',' ')\n",
    "\n",
    "# Make them lowercase and remove punctuation\n",
    "punc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x.lower()).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T02:30:50.257556Z",
     "start_time": "2019-12-24T02:30:50.253638Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get part of speech for lemmatization\n",
    "def get_wordnet_pos(word):\n",
    "    ''' \n",
    "    Map POS tag to first character lemmatize() accepts.\n",
    "    '''\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T02:30:50.263315Z",
     "start_time": "2019-12-24T02:30:50.259688Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lemmatization function\n",
    "def lemmatizer(text):\n",
    "    '''\n",
    "    Lemmatizes a given string.\n",
    "    '''\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in word_tokenize(text)]\n",
    "    lemmatized_text = ' '.join(tokens)\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T02:30:50.270810Z",
     "start_time": "2019-12-24T02:30:50.267415Z"
    }
   },
   "outputs": [],
   "source": [
    "# Named Entity function\n",
    "def named_entities(text):\n",
    "    '''\n",
    "    Replaces all named entities\n",
    "    before vectorization.\n",
    "    '''\n",
    "    for k, v in entities.items():\n",
    "        text = text.replace(k, v)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T02:30:50.278178Z",
     "start_time": "2019-12-24T02:30:50.273598Z"
    },
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# Display topic top words\n",
    "def display_topic_words(model, feature_names, no_top_words, topic_names=None):\n",
    "    '''\n",
    "    Display the top words for each topic.\n",
    "    '''\n",
    "    \n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic: \", ix)\n",
    "        else:\n",
    "            print(\"\\n\", ix, \"-\", topic_names[ix], \"\\n\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T02:30:50.283665Z",
     "start_time": "2019-12-24T02:30:50.280319Z"
    },
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "def top_movies_for_topic(topic):\n",
    "    return [plot_df[\"Title\"][x] for x in list(np.argsort(doc_topic[:,topic])[::-1][0:100])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Words and Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T02:30:50.290864Z",
     "start_time": "2019-12-24T02:30:50.285561Z"
    }
   },
   "outputs": [],
   "source": [
    "# Display topic top words/documents\n",
    "def display_topics(H, W, feature_names, documents, no_top_words, no_top_documents):\n",
    "    '''\n",
    "    Display top words and documents for each topics.\n",
    "    '''\n",
    "    for topic_idx, topic in enumerate(H):\n",
    "        print(f\"\\nTopic: {topic_idx}\\n\")\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        print(\"\\n\")\n",
    "        top_doc_indices = np.argsort( W[:,topic_idx] )[::-1][0:no_top_documents]\n",
    "        for doc_index in top_doc_indices:\n",
    "            print(documents[doc_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T02:30:50.296770Z",
     "start_time": "2019-12-24T02:30:50.293088Z"
    }
   },
   "outputs": [],
   "source": [
    "# Top topics for movie_id\n",
    "def movie_topics(movie):\n",
    "    '''\n",
    "    Get the top topics for the given movie_id.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    return [topics[x] for x in list(topic_vectors.iloc[movie_to_id[movie],:].sort_values(ascending = False).index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T02:30:50.302657Z",
     "start_time": "2019-12-24T02:30:50.299285Z"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize topic vectors so movie_topics works propertly\n",
    "def normalize_vector(vector):\n",
    "    '''\n",
    "    Normalize a vector with ranks.\n",
    "    '''\n",
    "    norm = vector.argsort()\n",
    "    ranks = np.empty_like(norm)\n",
    "    ranks[norm] = np.arange(len(vector))\n",
    "    return ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T02:30:50.308254Z",
     "start_time": "2019-12-24T02:30:50.304862Z"
    }
   },
   "outputs": [],
   "source": [
    "def top_movies_for_topic(topic):\n",
    "    return [plot_df[\"Title\"][x] for x in list(np.argsort(doc_topic[:,topic])[::-1][0:100])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spell Check for Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T02:30:50.313994Z",
     "start_time": "2019-12-24T02:30:50.310397Z"
    }
   },
   "outputs": [],
   "source": [
    "def spell_check(movie_input, movie_titles):\n",
    "    '''\n",
    "    Gives you the most likely movie name based on\n",
    "    what you type in.\n",
    "    '''\n",
    "    \n",
    "    most_similar = 0\n",
    "    for movie in movie_titles:\n",
    "        ratio = fuzz.ratio(movie_input, movie)\n",
    "        if ratio > most_similar:\n",
    "            most_similar = ratio\n",
    "            closest_movie = movie\n",
    "    return closest_movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recommend Movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T02:30:50.326483Z",
     "start_time": "2019-12-24T02:30:50.316657Z"
    }
   },
   "outputs": [],
   "source": [
    "def recommend_movie(movie_input, n_recs = 1):\n",
    "    '''\n",
    "    Recommends movie based on highest cosine similarity.\n",
    "    '''\n",
    "\n",
    "    if n_recs < 1:\n",
    "        return (\"You chose to receive 0 recommendations.\")\n",
    "    \n",
    "    # Empty list of ranks\n",
    "    ranks = []\n",
    "\n",
    "    # Retrieved individual movies by splitting at comma\n",
    "    movie_input = movie_input.split(\",\")\n",
    "\n",
    "    # Clean up white space for each entry\n",
    "    for idx in range(len(movie_input)):\n",
    "        movie_input[idx] = movie_input[idx].strip()\n",
    "\n",
    "    # For each movie in the list of inputted movies\n",
    "    for idx, movie in enumerate(movie_input):\n",
    "        \n",
    "        # Returns the closest movie title if typo\n",
    "        movie = spell_check(movie, movie_titles)\n",
    "        print(movie)\n",
    "        # Edits the entry in the movie input list\n",
    "        movie_input[idx] = movie\n",
    "\n",
    "        # Turn movie string into row index for movie\n",
    "        movie = movie_to_id[movie]\n",
    "        \n",
    "        # Cosine distances for the given movie to all others\n",
    "        dists = [dist[0] for dist in pairwise_distances(doc_topic, doc_topic[movie].reshape(1,-1))]\n",
    "\n",
    "        # Sort the distances from closest to furthest, excluding the movie itself, and retain movie ids\n",
    "        rec_movie_ids = np.argsort(dists)[1:]\n",
    "        \n",
    "        # Add this movie's ranks to the ranks list\n",
    "        ranks.append(rec_movie_ids)\n",
    "     \n",
    "    # Create a dictionary of \"average\" ranks per movie\n",
    "    rank_dict = {}\n",
    "    \n",
    "    # Loop through each movie and add the ranks up\n",
    "    for i in range(len(movie_input)):\n",
    "        for idx, movie in enumerate(ranks[i]):\n",
    "            try:\n",
    "                rank_dict[movie] += idx\n",
    "            except:\n",
    "                rank_dict[movie] = idx\n",
    "    \n",
    "    # Generate and return movie recommendation(s), and spell checked movie input\n",
    "    if n_recs == 1:\n",
    "        movie_recommendation = [id_to_movie[min(rank_dict, key = rank_dict.get)]]\n",
    "        return movie_recommendation\n",
    "    else:\n",
    "        movie_recommendations = [id_to_movie[x[0]] for x in sorted(list(rank_dict.items()), key = lambda x: x[1])][:int(n_recs)]\n",
    "        return movie_recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean movie titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T02:30:50.341119Z",
     "start_time": "2019-12-24T02:30:50.328463Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get rid of spaces on the ends of titles\n",
    "movie_df[\"Title\"] = movie_df[\"Title\"].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T02:30:50.349415Z",
     "start_time": "2019-12-24T02:30:50.343225Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fix movie title with typos\n",
    "movie_df.iloc[14640,1] = \"The Conjuring\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Puncutation, lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T02:31:00.529327Z",
     "start_time": "2019-12-24T02:30:50.351543Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clean punctuation\n",
    "movie_df[\"Plot\"] = movie_df[\"Plot\"].map(remove_apostrophes).map(remove_numbers).map(punc_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T02:55:02.338858Z",
     "start_time": "2019-12-24T02:31:14.361488Z"
    }
   },
   "outputs": [],
   "source": [
    "movie_df[\"Plot\"] = movie_df[\"Plot\"].apply(lambda x: lemmatizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T02:58:21.759930Z",
     "start_time": "2019-12-24T02:58:21.603135Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"pickles/movie_df\",\"wb\") as file:\n",
    "    pickle.dump(movie_df,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickled Starting Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T02:58:22.802362Z",
     "start_time": "2019-12-24T02:58:22.656107Z"
    }
   },
   "outputs": [],
   "source": [
    "# Open the pickle\n",
    "with open(\"pickles/movie_df\",\"rb\") as file:\n",
    "    movie_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T02:58:23.035726Z",
     "start_time": "2019-12-24T02:58:23.005735Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop movies with short plots to avoid overfitting\n",
    "movie_df = movie_df[movie_df[\"Plot\"].apply(lambda x: len(x) > 400)].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T02:58:25.084489Z",
     "start_time": "2019-12-24T02:58:25.076710Z"
    }
   },
   "outputs": [],
   "source": [
    "# Named Entity Declaration\n",
    "entities = {' new york ':' new_york ',\n",
    "            ' los angeles ':' los_angeles ',\n",
    "            ' van helsing ':' van_helsing ',\n",
    "            ' high school ':' high_school ',\n",
    "            ' united state ':' united_states ',\n",
    "            ' united states ':' united_states ',\n",
    "            ' hong kong ':' hong_kong ',\n",
    "            ' kingdom ':' king ',\n",
    "            ' world war ':' world_war ',\n",
    "            ' world_war ii ': ' world_war ',\n",
    "            ' gun shot ':' gun_shot ',\n",
    "            ' performance ':' perform ',\n",
    "            ' commit suicide ':' commit_suicide ',\n",
    "            ' central park ':' central_park ',\n",
    "            ' police officer ':' police_officer ',\n",
    "            ' steal money ':' steal_money ',\n",
    "            ' college student ':' college_student ',\n",
    "            ' set free ':' set_free ',\n",
    "            ' haunt house ':' haunted_house ',\n",
    "            ' marry ':' marriage ',\n",
    "            ' investigate ':' investigation ',\n",
    "            ' develops ':' develop ',\n",
    "            ' teacher ':' teach ',\n",
    "            ' form story ':' form_story ',\n",
    "            ' dr ':' doctor ',\n",
    "            ' best friend ':' best_friend ',\n",
    "            ' childhood friend ':' childhood_friend ',\n",
    "            ' close friend ':' close_friend ',\n",
    "            ' car accident ':' car_accident ',\n",
    "            ' commits suicide ':' suicide ',\n",
    "            ' commit suicide ':' suicide ',\n",
    "            ' happily ':' happy ',\n",
    "            ' small town ':' small_town ',\n",
    "            ' writer ':' write ',\n",
    "            ' writes ':' write ',\n",
    "            ' heart attack ':' heart_attack ',\n",
    "            ' die ':' death ',\n",
    "            ' dead ':'death ',\n",
    "            ' small town ':' small_town ',\n",
    "            ' player ':' play ',\n",
    "            ' night club ':' night_club ',\n",
    "            ' singer ':' sing ',\n",
    "            ' police station ':' police_station ',\n",
    "            ' destroyed ':' destroy ',\n",
    "            ' competition ':' compete ',\n",
    "            ' cross country ':' cross_country ',\n",
    "            ' marries ':' marriage ',\n",
    "            ' air force ':' air_force ',\n",
    "            ' married ':' marriage ',\n",
    "            ' newly wed ':' newly_wed ',\n",
    "            ' romantically ':' romantic ',\n",
    "            ' seek revenge ':' seek_revenge ',\n",
    "            ' reading ':' read ',\n",
    "            ' sings ':' sing '\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T02:58:28.875730Z",
     "start_time": "2019-12-24T02:58:25.657625Z"
    }
   },
   "outputs": [],
   "source": [
    "# Named Entity Application\n",
    "movie_df[\"Plot\"] = movie_df[\"Plot\"].apply(lambda x: named_entities(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T02:58:28.885425Z",
     "start_time": "2019-12-24T02:58:28.878391Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add general English stopwords without apostrophes\n",
    "more_stopwords = []\n",
    "\n",
    "for word in list(stopwords.words('english')):\n",
    "    more_stopwords.append(word.replace('\\'',''))\n",
    "\n",
    "# Join's the stop words above to the standard English list\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(more_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T02:58:30.961249Z",
     "start_time": "2019-12-24T02:58:30.940692Z"
    }
   },
   "outputs": [],
   "source": [
    "# Misc Category\n",
    "other = ['rama','later','night','away','manner','door',\n",
    "         'left','new','away','way','process','purpose','sens',\n",
    "         'el','pas','section','good','multiple','attractive',\n",
    "         'favorite','calcutta','interested','repeatedly','thing',\n",
    "         'von','time','leaf','dinner','babu','big','inside',\n",
    "         'outside','window','rao','day','hand','hard','end',\n",
    "         'yearold','face','second','unable','reason','happens'\n",
    "         ,'meantime','problem','life','true','past','care','sight'\n",
    "         ,'eventually','year','ago','long','old','lose','present',\n",
    "         'great','need','age','soon','head','happy','honest','head',\n",
    "         'arm','role','department','result','room','wall','sudden',\n",
    "         'suddenly','house','hall','different','elder','beautiful',\n",
    "         'young','handsome','real','actually','truth','really','mistake',\n",
    "         'set','large','despite','final','trip','store','east','park',\n",
    "         'small','social','bad','couple','home','mate','exact','london',\n",
    "         'india','paris','case','fall']\n",
    "\n",
    "# Verbs\n",
    "verb = ['come','leave','stay','say','tell','make','help','meet',\n",
    "        'know','like','asks','use','want','follow','stake','kill',\n",
    "        'pull','try','visit','return','let','stop','start','ask',\n",
    "        'miss','lot','talk','reveals','run','begin','explains',\n",
    "        'decides','change','open','run','walk','attempt','plan',\n",
    "        'refuse','complete','decision','inform','pick','confuse',\n",
    "        'attach','parking','approach','dislike','raise','lift',\n",
    "        'increase','choose','dy','rest','look','rid','look',\n",
    "        'realizes','spend','arrives','fail','turn','hold',\n",
    "        'confronts','turn','realize','chase','knock','grab',\n",
    "        'cause','throw','agrees','include','cause','manages',\n",
    "        'arrive','happen','decide','reach','ride','fall','appear',\n",
    "        'wake','watch','eat','cut','lock','attack','watch','hears',\n",
    "        'wish','revolves','sends','play','sent','feel','think','focus',\n",
    "        'described','save','share','attend','board','cross','accompany',\n",
    "        'grow','save','lead','played','join','involve','involves',\n",
    "        'receives','love']\n",
    "\n",
    "# People\n",
    "people = ['man','woman','girl','boy','sir','madam','professor',\n",
    "         'guy','doc','boss','mr','person','lady','men']\n",
    "         \n",
    "# Names\n",
    "name = ['michael','peter','sam','john','jane','max','tim',\n",
    "        'curtis','jimmy','charlie','elizabeth','mike','paul',\n",
    "        'nick','jimmy','eddie','tony','henry','paul','joes',\n",
    "        'joe','emily','lily','amy','edward','frank','johnny',\n",
    "        'helen','ben','diane','frank','johnny','martin','george',\n",
    "        'anne','lucy','linda','leo','carl','alice','bobby',\n",
    "        'martha','tom','jerry','rachel','ross','jenny','ann',\n",
    "        'jennifer','lloyd','raj','walter','james','mary','steve',\n",
    "        'billy','norman','ann','ray','jonathan','arthur','nikki',\n",
    "        'frederick','jason','jessica','david','mia','katherine',\n",
    "        'judy','steven','julie','susan','cynthia','shane','allan',\n",
    "        'alex','sally','kim','lou','victor','ash','harris','wendy',\n",
    "        'adam','grace','jim','glen','terry','al','margaret','carrie',\n",
    "        'danny','alan','robert','christine','jack','thomas','ralph',\n",
    "        'charlotte','nancy','simon','jake','pete','joseph','jacob',\n",
    "        'hank','kelly','anna','stephen','dan','sean','larry','sarah',\n",
    "        'karl','jackie','carter','scott','pete','harry','kate','eve',\n",
    "        'phil','dean','cole','graham','jordan','phyllis','bob','sue',\n",
    "        'rita','michelle','diana','mark','daniel','matt','lisa','duke',\n",
    "        'morgan','marie','raymond','karen','maria','todd','janet','fred',\n",
    "        'richard','annie','drake','julia','francis','charles','stewart',\n",
    "        'richards','olivia','lawrence','lee','jeff','ellen','andy','andrew',\n",
    "        'ruth','ed','miller','jones','taylor','kumar','shankar','ajay',\n",
    "        'signh','prakash','prasad','joan','rahul','li','chris','singh',\n",
    "        'khan','mohan','krishna','ravi','rajah','anand','vijay','kapoor',\n",
    "        'raja','radha','lakshmi']\n",
    "\n",
    "# Family\n",
    "family = ['family','son','brother','sister','child','wife','daughter',\n",
    "          'mother','husband','father','parent','uncle','cousin','grandfather',\n",
    "          'aunt']\n",
    "\n",
    "add_stop_words = other + verb + people + name + family\n",
    "\n",
    "# Join's the stop words above to the standard English list\n",
    "stop_words = stop_words.union(add_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T02:59:27.744409Z",
     "start_time": "2019-12-24T02:58:33.601092Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the vectorizer object\n",
    "vectorizer = TfidfVectorizer(ngram_range = (1,3), stop_words = stop_words, min_df = .01, binary = False)\n",
    "\n",
    "# Create the doc_word sparse matrix\n",
    "doc_word = vectorizer.fit_transform(movie_df[\"Plot\"])\n",
    "\n",
    "# Create a dataframe for easy labeleled viewing\n",
    "doc_word_df = pd.DataFrame(doc_word.toarray(), columns = vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T02:59:38.177060Z",
     "start_time": "2019-12-24T02:59:27.746718Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create and NMF object with 35 topics\n",
    "nmf = NMF(n_components = 35)\n",
    "\n",
    "# Fit the doc_word sparse matrix\n",
    "doc_topic = nmf.fit_transform(doc_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T03:02:57.693304Z",
     "start_time": "2019-12-24T03:02:57.687806Z"
    },
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# Topic Names\n",
    "topics = \\\n",
    "{\n",
    "0:\"Relationships / Sex\",\n",
    "1:\"Marriage\",\n",
    "2:\"Modern War\",\n",
    "3:\"Police\",\n",
    "4:\"Village\",\n",
    "5:\"Medical\",\n",
    "6:\"Royalty\",\n",
    "7:\"Acting\",\n",
    "8:\"School\",\n",
    "9:\"Money\",\n",
    "10:\"Sports\",\n",
    "11:\"Gangs\",\n",
    "12:\"Ships / Water\",\n",
    "13:\"Music\",\n",
    "14:\"Western\",\n",
    "15:\"Driving\",\n",
    "16:\"College / Relationships\",\n",
    "17:\"Pregnancy / Adoption\",\n",
    "18:\"Office Life\",\n",
    "19:\"Tales / Journeys\",\n",
    "20:\"Writing\",\n",
    "21:\"Horse-Racing / Ranch\",\n",
    "22:\"Aliens / Destruction\",\n",
    "23:\"Train / Travel\",\n",
    "24:\"Murder / Crime\",\n",
    "25:\"Bollywood / Mobster\",\n",
    "26:\"Combat\",\n",
    "27:\"Airplanes\",\n",
    "28:\"Prison / Justice\",\n",
    "29:\"City Life\",\n",
    "30:\"Heist\",\n",
    "31:\"Death / Spirits\",\n",
    "32:\"Animals\",\n",
    "33:\"Dancing / Performance\",\n",
    "34:\"Drug Crime\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display Topic Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T03:02:58.660178Z",
     "start_time": "2019-12-24T03:02:58.634237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0 - Relationships / Sex \n",
      "\n",
      "relationship, party, sex, apartment, date, hotel, kiss, affair, wedding, invite, boyfriend, break, divorce, feeling, morning\n",
      "\n",
      " 1 - Marriage \n",
      "\n",
      "marriage, wedding, rich, wealthy, proposal, divorce, arrange, lover, accepts, live, bride, finally, shock, affair, pregnant\n",
      "\n",
      " 2 - Modern War \n",
      "\n",
      "german, army, soldier, war, british, officer, camp, general, american, colonel, troop, order, prisoner, command, nazi\n",
      "\n",
      " 3 - Police \n",
      "\n",
      "police, arrest, inspector, prison, jail, escape, criminal, police_officer, crime, release, drug, commissioner, officer, police inspector, sentence\n",
      "\n",
      " 4 - Village \n",
      "\n",
      "village, villager, temple, land, people, landlord, local, farmer, city, chief, respect, poor, priest, form, teach\n",
      "\n",
      " 5 - Medical \n",
      "\n",
      "doctor, hospital, patient, medical, nurse, treat, cure, surgery, treatment, psychiatrist, mental, death, experiment, work, scientist\n",
      "\n",
      " 6 - Royalty \n",
      "\n",
      "king, prince, queen, princess, palace, lord, castle, royal, minister, sword, forest, court, defeat, order, louis\n",
      "\n",
      " 7 - Acting \n",
      "\n",
      "film, movie, character, star, director, actor, scene, actress, studio, producer, hollywood, production, plot, cast, feature\n",
      "\n",
      " 8 - School \n",
      "\n",
      "school, student, teach, class, high_school, principal, classmate, dance, kid, master, study, bully, boarding, art, education\n",
      "\n",
      " 9 - Money \n",
      "\n",
      "money, bank, pay, steal, rich, debt, sell, cash, million, buy, gamble, robbery, loan, earn, rob\n",
      "\n",
      " 10 - Sports \n",
      "\n",
      "team, game, coach, win, football, match, baseball, compete, score, ball, season, star, sport, field, member\n",
      "\n",
      " 11 - Gangs \n",
      "\n",
      "gang, member, gang member, leader, gangster, robbery, fight, rival, criminal, bank, group, rob, crime, bos, gun\n",
      "\n",
      " 12 - Ships / Water \n",
      "\n",
      "ship, crew, captain, aboard, sea, sail, navy, officer, boat, rescue, passenger, command, sink, planet, water\n",
      "\n",
      " 13 - Music \n",
      "\n",
      "sing, song, music, perform, band, dance, stage, club, concert, star, record, musical, career, audience, nightclub\n",
      "\n",
      " 14 - Western \n",
      "\n",
      "town, sheriff, local, ranch, gun, shoot, townspeople, deputy, gold, mayor, judge, farm, posse, land, texas\n",
      "\n",
      " 15 - Driving \n",
      "\n",
      "car, drive, driver, truck, road, shoot, crash, gun, steal, escape, body, hit, garage, gas, bus\n",
      "\n",
      " 16 - College / Relationships \n",
      "\n",
      "friend, group, friendship, fight, movie, finally, chennai, best_friend, search, kid, place, girlfriend, hospital, accident, realises\n",
      "\n",
      " 17 - Pregnancy / Adoption \n",
      "\n",
      "baby, pregnant, birth, born, hospital, pregnancy, adopt, death, infant, orphanage, abandon, live, nurse, twin, labor\n",
      "\n",
      " 18 - Office Life \n",
      "\n",
      "job, work, company, business, bos, office, offer, factory, worker, owner, manager, dream, sell, hire, firm\n",
      "\n",
      " 19 - Tales / Journeys \n",
      "\n",
      "story, form, form story, movie, rich, character, people, lover, journey, place, relationship, twist, tale, event, villain\n",
      "\n",
      " 20 - Writing \n",
      "\n",
      "write, letter, book, read, novel, write letter, death, note, publish, newspaper, suicide, word, believe, page, copy\n",
      "\n",
      " 21 - Horse-Racing / Ranch \n",
      "\n",
      "horse, race, win, bet, ranch, compete, farm, track, finish, prize, driver, champion, victory, gambler, buy\n",
      "\n",
      " 22 - Aliens / Destruction \n",
      "\n",
      "dog, cat, animal, pet, tree, escape, food, hunt, water, white, hole, owner, wood, bear, tail\n",
      "\n",
      " 23 - Train / Travel \n",
      "\n",
      "island, boat, native, sea, treasure, water, group, beach, escape, swim, sail, ocean, giant, rescue, cave\n",
      "\n",
      " 24 - Murder / Crime \n",
      "\n",
      "murder, killer, death, investigation, detective, body, suspect, crime, victim, murderer, evidence, witness, commit, lawyer, shoot\n",
      "\n",
      " 25 - Bollywood / Mobster \n",
      "\n",
      "ram, lucky, indian, goon, master, major, movie, birth, live, twin, rescue, place, promise, shock, grandson\n",
      "\n",
      " 26 - Combat \n",
      "\n",
      "tommy, fight, kid, boyfriend, round, football, judge, ring, san, manager, club, win, hope, nightclub, smith\n",
      "\n",
      " 27 - Airplanes \n",
      "\n",
      "agent, terrorist, bomb, fbi, president, secret, escape, government, hotel, drug, spy, assassin, information, security, mission\n",
      "\n",
      " 28 - Prison / Justice \n",
      "\n",
      "train, station, passenger, railway, train station, travel, journey, track, indian, engine, ticket, catch, express, fight, tunnel\n",
      "\n",
      " 29 - City Life \n",
      "\n",
      "city, new_york, new_york city, apartment, street, mayor, building, travel, newspaper, living, reporter, live, american, editor, country\n",
      "\n",
      " 30 - Heist \n",
      "\n",
      "japanese, chinese, japan, fight, american, china, soldier, master, marine, martial, hong_kong, defeat, art, battle, army\n",
      "\n",
      " 31 - Death / Spirits \n",
      "\n",
      "college, student, study, university, minister, medical, education, football, principal, election, rich, graduate, win, classmate, class\n",
      "\n",
      " 32 - Animals \n",
      "\n",
      "pilot, fly, aircraft, plane, flight, land, crash, air, mission, base, passenger, air_force, fighter, airport, test\n",
      "\n",
      " 33 - Dancing / Performance \n",
      "\n",
      "alien, earth, human, power, creature, world, monster, destroy, planet, group, escape, space, fight, body, battle\n",
      "\n",
      " 34 - Drug Crime \n",
      "\n",
      "ghost, spirit, death, haunt, body, possess, christmas, curse, soul, suicide, mansion, priest, evil, strange, accident\n"
     ]
    }
   ],
   "source": [
    "# Top 15 terms per topic for evaluation\n",
    "display_topic_words(nmf, vectorizer.get_feature_names(), 15, topic_names = topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Individual Movie Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T03:03:05.748717Z",
     "start_time": "2019-12-24T03:03:02.262446Z"
    }
   },
   "outputs": [],
   "source": [
    "# Normalized topic vectors\n",
    "topic_vectors = []\n",
    "for i in range(doc_topic.shape[1]):\n",
    "    topic_vectors.append(normalize_vector(np.array(pd.DataFrame(doc_topic)[i])))\n",
    "    \n",
    "topic_vectors = pd.DataFrame(topic_vectors).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Movie IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T03:03:05.772599Z",
     "start_time": "2019-12-24T03:03:05.750918Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a list of movie ids and movie titles\n",
    "movie_ids = movie_df[\"Title\"].index.tolist()\n",
    "movie_titles = movie_df[\"Title\"].tolist()\n",
    "\n",
    "# Create dictionarys to access them both ways\n",
    "movie_to_id = {}\n",
    "id_to_movie = {}\n",
    "\n",
    "# Populate movie to id\n",
    "for idx in range(len(movie_titles)):\n",
    "    movie_to_id[movie_titles[idx]] = movie_ids[idx]\n",
    "\n",
    "# Populate id to movie\n",
    "for idx in range(len(movie_titles)):\n",
    "    id_to_movie[movie_ids[idx]] = movie_titles[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T03:03:06.843616Z",
     "start_time": "2019-12-24T03:03:06.817608Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('pickles/jupyter_pickles','wb') as file:\n",
    "    pickle.dump(movie_to_id, file)\n",
    "    pickle.dump(id_to_movie, file)\n",
    "    pickle.dump(movie_titles, file)\n",
    "    pickle.dump(doc_topic, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T03:04:42.005284Z",
     "start_time": "2019-12-24T03:04:41.675866Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get Out\n",
      "The Ring\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Himalaya Singh']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_movie('get out, the ring, 3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "269.969px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
